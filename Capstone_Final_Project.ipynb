{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\mypython\\anaconda3\\envs\\py36\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in c:\\mypython\\anaconda3\\envs\\py36\\lib\\site-packages (from geopy) (1.50)\n",
      "Folium installed\n",
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import requests # library to handle requests\n",
    "import pandas as pd # library for data analsysis\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import random # library for random number generation\n",
    "\n",
    "\n",
    "!pip install geopy\n",
    "from geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values\n",
    "\n",
    "# libraries for displaying images\n",
    "from IPython.display import Image \n",
    "from IPython.core.display import HTML \n",
    "    \n",
    "# tranforming json file into a pandas dataframe library\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#! pip install folium==0.5.0\n",
    "import folium # plotting library\n",
    "\n",
    "print('Folium installed')\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOURSQUARE IDs \n",
    "CLIENT_ID = '3S1PFTNL1HAFWEYMYHRLCVZVWN1DSK0IPHKSX5JXAGR3E1ML' # your Foursquare ID\n",
    "CLIENT_SECRET = 'V4IEPFK4NJZZER4LJA33R2LX3S5KRBHVECAFZJDVTLFZUTBP' # your Foursquare Secret\n",
    "VERSION = '20180604'\n",
    "LIMIT = 30\n",
    "#print('Your credentails:')\n",
    "#print('CLIENT_ID: ' + CLIENT_ID)\n",
    "#print('CLIENT_SECRET:' + CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject: Social places vs Covid19 \n",
    "Question I want to address is : Can we see relationship between large cities' social places (such as  bar, restaurant, cinema, theatre, ... ) and postive Covid19 cases in France ?\n",
    "How to: I'm going to get  the list of biggest cities on France, get the numbers about the Covid19 cases in France, get the top most visited social places information from FourSquare for these cities. Aggregating these different data sets, I will leverage the clustering techniques to highlight similarities and hopefully get some insight of my original question. \n",
    "My assumption is that, the more we would have Social places the highest would be the number of Covid positive cases.. \n",
    "\n",
    "\n",
    "### Business problem\n",
    "It is obvious I believe the economical impact of closing everything down. There are many different shopts being closed eventhough the medical protective measure were in place. Economical impact oculd be even bigger than the medical impact. Looking at the problem by mixing the most visited places and types of places vs the number of cases can shade some lights on what are the places we could open again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data: \n",
    "I will gather  Covid related data: numbers of positive cases by region/department, number of Covid test  per region/departement to reduce the bias of number of positive vs number of tested.\n",
    "<br>I will list of cities from which I will take the largest ( most representative ) for each department\n",
    "<br>I will use the Foursquare API to get the \"Social\" places around the biggest cities \n",
    "\n",
    "\n",
    "#### Details on each data set to be used for this study:\n",
    "\n",
    "#### Covid stats : pulling data from the governement portal\n",
    "\n",
    "From May to December 2020\n",
    "https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19/\n",
    "file: https://static.data.gouv.fr/resources/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19/20201211-192020/sp-pos-quot-dep-2020-12-11-19h20.csv\n",
    "url pointing to daily updates ( permanent url for latest info ): https://www.data.gouv.fr/fr/datasets/r/406c6a23-e283-4300-9484-54e78c8ae675\n",
    "where:\n",
    "- dep\tis the departement ,\n",
    "- jour is the date of data collection \t\n",
    "- P\tis positive tested results of the given date\n",
    "- T\tis tested in total of the given date\n",
    "- cl_age90\t: this attributes is not clearly explain I will then ignore it.\n",
    "- pop is the total  population in that departement \n",
    "\n",
    "\n",
    "#### French Cities  list per departments: : pulling data from the governement portal\n",
    "\n",
    "https://www.data.gouv.fr/fr/datasets/regions-departements-villes-et-villages-de-france-et-doutre-mer/\n",
    "==> direct link to the data csv data set:https://www.data.gouv.fr/fr/datasets/r/a8adb872-dfac-46b1-9885-da8ca7d2d931, from where I will use the cities.csv \n",
    "where the below attributes are:\n",
    "- id : unique ID\n",
    "- department_cod : numeric code of the department where the city is\n",
    "- insee_code : code being used for statistic city identifier\n",
    "- zip_code : Postal code of the city\n",
    "- name : Name of the department \n",
    "- slug : Name of the department without special character inside\n",
    "- gps_lat\tgps_lng  : coordinates of the City\n",
    "\n",
    "largest cities (  based on population ):\n",
    "https://fr.wikipedia.org/wiki/Liste_des_communes_de_France_les_plus_peupl%C3%A9es\n",
    "I'll take only the Year 2017 as it is the most recent \n",
    "From that url, I will use the table  named 'Communes de plus de 30 000 habitants' , and I will used the INSEE code to merge de data set with the previous data set so I can restrict my study to largest cities only.\n",
    "\n",
    "\n",
    "#### Crisis Contextual information , to bring more insights \n",
    "Differents action in lockdown situation in France. I may input these information to refine observation and see if there are drops in the number of positive Covid tests\n",
    "https://www.lemonde.fr/les-decodeurs/article/2020/05/12/coronavirus-de-la-chauve-souris-au-deconfinement-la-chronologie-de-la-pandemie_6039448_4355770.html\n",
    "https://fr.wikipedia.org/wiki/Chronologie_de_la_pand%C3%A9mie_de_Covid-19_en_France\n",
    "I will use these information on a visual representation to see the different influence ( or not ) of the lockdown situation on the positive tests, assuming I have enough data to cross at the same period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Covid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "covid_url = \"https://www.data.gouv.fr/fr/datasets/r/406c6a23-e283-4300-9484-54e78c8ae675\"\n",
    "\n",
    "src = requests.get(covid_url).content  # get raw formatted content data from that url\n",
    "covid_data = pd.read_csv(io.StringIO(src.decode('utf-8')),sep=';',low_memory=False) # pass to pandas read_csv a 'file-like' object the raw we got from prev step\n",
    "\n",
    "## Convert the jour to a known date formate for later use\n",
    "covid_data.jour = pd.to_datetime(covid_data.jour)\n",
    "\n",
    "##Calculate here the ratio Postive vs Tested to have a relative value to compare afterwards\n",
    "covid_data['PtoT_ratio'] = covid_data['P']/covid_data['T']\n",
    "covid_data['PtoT_ratio'].fillna(0,inplace=True) # put 0 whenever there is no data available in P or T\n",
    "## Calculate positive vs the population of a given Department \n",
    "covid_data['PtoPop_ratio'] = covid_data['P']/covid_data['pop']\n",
    "\n",
    "# This data set contains all the data related to Covid tests since May 2020\n",
    "display (covid_data.tail())\n",
    "\n",
    "# How many departement do we have:\n",
    "print (\" We have \",len(covid_data.dep.unique()),\" département in Covid dataset\")\n",
    "departement_covid = covid_data.dep.unique()\n",
    "\n",
    "\n",
    "### Refine this dataset to keep only the average of PtoPop_ratio, as this is the data we want to  include in our study . Meaning what is the average Positive cases detection in regards to the Social places\n",
    "covid_data_refine = covid_data.groupby(['dep']).agg({'PtoT_ratio':'mean'}).reset_index()\n",
    "\n",
    "display (covid_data_refine.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Last date entry from my Covid data source \n",
    "display ('Latest data entry date we have in the covid data ? ')\n",
    "display (covid_data['jour'].unique()[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the coordinate of cities in France\n",
    "### this dataset have been taken from the zip availabile in https://www.data.gouv.fr/fr/datasets/r/a8adb872-dfac-46b1-9885-da8ca7d2d931 , zip file which contains other file so I extracted that specific one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "localfile = \"C:/Users/JULIENSauvanet/Documents/JSAUVANET/Coursera/Projects/Coursera_Capstone/cities.csv\"\n",
    "french_cities_coordinates = pd.read_csv(localfile,sep=',',low_memory=False) # pass to pandas read_csv a 'file-like' object the raw we got from prev step . \n",
    "\n",
    "## Format the zip code to int\n",
    "## Before I need to take care of those without Zip code from the TOM/DOM\n",
    "french_cities_coordinates.zip_code.fillna('00000',inplace=True)\n",
    "french_cities_coordinates.zip_code.astype(np.int64)\n",
    "\n",
    "french_cities_coordinates.dtypes\n",
    "display (french_cities_coordinates.head())\n",
    "# How many departement do we have:\n",
    "print (\" We have \",len(french_cities_coordinates.department_code.unique()),\" département in City Coordinates dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset to providing the largest cities in France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://fr.wikipedia.org/wiki/Liste_des_communes_de_France_les_plus_peupl%C3%A9es\"\n",
    "tables = pd.read_html(url, encoding='utf-8') # Returns list of all tables on page\n",
    "# First table of that page is what we need .. \n",
    "big_french_cities = tables[0] \n",
    "\n",
    "# Cleaning the old Population statscolumns as I need only the most recent year ( 7th columns)\n",
    "nbofcols= big_french_cities.shape[1]\n",
    "col = 0\n",
    "while col < 7:\n",
    "    nbofcols = nbofcols - 1\n",
    "    big_french_cities.drop(big_french_cities.columns[nbofcols], axis='columns',inplace=True)\n",
    "    big_french_cities.shape[1]\n",
    "    col = col +1\n",
    "\n",
    "## Cleaning the Multiu Columns level\n",
    "big_french_cities.columns = big_french_cities.columns.droplevel(1)\n",
    "\n",
    "## Cleaning the Population numbers format to get rid of the x08F ==> str.replace('[^\\x00-\\x8F]', '')\n",
    "big_french_cities['Population légale'] = big_french_cities['Population légale'].str.replace('[^\\x00-\\x8F]', '')\n",
    "## get rid of non standard format in Population légale , found sometime there are info into brakets.. need to clean this up\n",
    "big_french_cities['Population légale'] = big_french_cities['Population légale'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "big_french_cities['Population légale'] = big_french_cities['Population légale'].astype(np.int64)\n",
    "\n",
    "#print (big_french_cities.dtypes)\n",
    "display (big_french_cities.tail())\n",
    "\n",
    "# Do we have one \"big\" city for each department ?\n",
    "print (\" We have \",len(big_french_cities['Département'].unique()),\" département in Big cities dataset\")\n",
    "\n",
    "departement_big_french_cities = big_french_cities['Département'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Merge two data sets: largest cities and list of cities with their coordinates ( based on Insee Code)\" )\n",
    "## big_french_cities => CodeInsee & french_cities_coordinates => insee_code\n",
    "# rename header for the merge operation\n",
    "big_french_cities.rename(index=str, columns={\"CodeInsee\": \"insee_code\"}, inplace = True)\n",
    "\n",
    "# merge our datafile wth the coordinate dataframe\n",
    "fr_cities = pd.merge(big_french_cities, french_cities_coordinates, on='insee_code', how='inner') # Join info on PostalCode which is our unique value, common to both dataframe\n",
    "\n",
    "# remove duplicates if any - I've found that multiple lines appears for same Commune with with different gps coordinates ( having multiple zip code within same City). which is useless in our case\n",
    "fr_cities.drop_duplicates(subset=['insee_code','Commune','Département','department_code'],inplace=True)\n",
    "\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "#Restrict the list for testing purposes\n",
    "#tmp = fr_cities.loc[(fr_cities.Commune.isin(['Nice','Paris','Marseille','Lille']))] ##Limiting to Nice for now to preserve FourSquare calls while testing\n",
    "#fr_cities = tmp\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "#########################################\n",
    "\n",
    "# drop the unwanted column\n",
    "fr_cities = fr_cities.drop(['Rang2020','Statut','id'],axis=1)\n",
    "\n",
    "print (\" I have a list of \" + str(fr_cities.shape[0]) + \" cities, I need only the biggest cities per departement.\")\n",
    "print (\" This is concerning \" + str(len(fr_cities['Département'].unique())) + \" departement.\")\n",
    "\n",
    "# keep only the largest city for each department :\n",
    "# by doing: sort by Population légale and drop duplicate keep only first occurence :\n",
    "tmp = fr_cities.sort_values(by=['Population légale'],ascending=False)\n",
    "fr_cities = tmp.drop_duplicates(subset=['Département'], keep='first')\n",
    "\n",
    "print (\" I have now a reduced list of \" + str(fr_cities.shape[0]) + \" cities, keeping largest per department.\")\n",
    "print (\" This is concerning \" + str(len(fr_cities['Département'].unique())) + \" departement.\")\n",
    "display (fr_cities.tail())\n",
    "\n",
    "\n",
    "fr_cities_departement = fr_cities['Département'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What departement are we missing : i.e where there is no \"big cities\". These departement cannot be included in this study as there is no official data source found for them\n",
    "# Relation between Insee Code and Departement code is : Departement are the first digit of the INSEE code . e.g all Insee code starting with 06xxxx relates to 06 Departement code\n",
    "# from this , search what INSEE code is missing in the big cities dataset\n",
    "departement_big_french_cities = big_french_cities.insee_code.astype(str).str[0:2].unique()\n",
    "\n",
    "display (HTML(\" Departement which will are considered : <br> Others - as we don't have big city information on them - are not going to be included \"))\n",
    "print ([x for x in fr_cities_departement if x not in departement_big_french_cities])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the famous social places for each of our  largest cities in France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Initialize the Foursquare data:\n",
    "CLIENT_ID = '3S1PFTNL1HAFWEYMYHRLCVZVWN1DSK0IPHKSX5JXAGR3E1ML' # your Foursquare ID\n",
    "CLIENT_SECRET = 'V4IEPFK4NJZZER4LJA33R2LX3S5KRBHVECAFZJDVTLFZUTBP' # your Foursquare Secret\n",
    "VERSION = '20180604'\n",
    "LIMIT = 80\n",
    "# function to get the venues for each cities \n",
    "# cities are taken from fr_cities , based on gps_lat,gps_lng for each Commune\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=20000):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results ])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Commune', \n",
    "                  'Commune Latitude', \n",
    "                  'Commune Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    ## filter out the Venues category we don't want .. we want to keep only what we defined as social places in the below list\n",
    "    \n",
    "    ## JSA update 27 Dec : make the same study without filters on social places to see whether cluster observation are differents \n",
    "    ### JSA update 27 Dec social_places = ['Restaurant','Coffee','Café','Tea Room','Bar','Bistro','Cinema']\n",
    "    ### JSA update 27 Dec nearby_venues = nearby_venues.loc[nearby_venues['Venue Category'].str.contains('|'.join(social_places))]\n",
    "    \n",
    "    return(nearby_venues)\n",
    "\n",
    "\n",
    "# Call the function for all our Cities and get back a dataframe with all the Venues\n",
    "fr_cities_venues = getNearbyVenues(names=fr_cities['Commune'],\n",
    "                                   latitudes=fr_cities['gps_lat'],\n",
    "                                   longitudes=fr_cities['gps_lng']\n",
    "                                  )\n",
    "\n",
    "display (fr_cities_venues.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  I'm now having the count of social places within the given radius of my biggest cities. \n",
    "\n",
    "data set <b>fr_cities_venues_grouped</b> is containing the list of Commune, with their coordinates and the number of Social places found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print (\"What are the venue category we have \" )\n",
    "#print (fr_cities_venues['Venue Category'].unique())\n",
    "\n",
    "# Count Social Venues per City\n",
    "print (\"Number of venues per largest city in each departement  \" )\n",
    "fr_cities_venues_grouped = fr_cities_venues.groupby(['Commune','Commune Latitude','Commune Longitude'])['Venue'].count().reset_index()\n",
    "display (fr_cities_venues_grouped)\n",
    "# Count per Venue category\n",
    "print (\"Count of venues per largest city in each departement - group by category \" )\n",
    "display (fr_cities_venues.groupby(['Commune','Commune Latitude','Commune Longitude','Venue Category']).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include  Covid data and merge them with the Cities data\n",
    "#### To be able to have a visual representation of the Covid data, I'm going to use the largest City of each department , as the Covid information are based on department. \n",
    "#### This is the full data set I'm going to use for clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display (covid_data.head())\n",
    "#display (fr_cities.head())\n",
    "\n",
    "print (\"Merge two data sets: Cities data set and Covid data( based on Department 2 digits number )\" )\n",
    "## covid_data => dep & fr_cities => Département\n",
    "# merge our datafiles with the coordinate dataframe\n",
    "covid_city_data = pd.merge(covid_data_refine, fr_cities, left_on='dep', right_on='department_code', how='inner') # Join info Departement info\n",
    "\n",
    "#display (covid_city_data)\n",
    "\n",
    "## I then keep only the largest City of each department as representative data\n",
    "myminpop = 10000\n",
    "print (\"covid_city_data before drop \",covid_city_data.shape)\n",
    "covid_city_data.drop(covid_city_data[covid_city_data['Population légale'] < myminpop].index, inplace=True)\n",
    "print (\"covid_city_data AFTER drop \",covid_city_data.shape)\n",
    "display (covid_city_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data set build : to have all the data on a single data set:  Covid data , Cities and Venues informations\n",
    "merge the fr_cities_venues_grouped with covid_city_data on the Commune name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print (fr_cities_venues_grouped.dtypes)\n",
    "#print (covid_city_data.dtypes)\n",
    "\n",
    "final_dataset = pd.merge(fr_cities_venues_grouped, covid_city_data, on='Commune', how='inner') # Join info Departement info\n",
    "# cast the output format the covid data to keep less float digits \n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "print (final_dataset.shape)\n",
    "display (final_dataset)\n",
    "\n",
    "#print (final_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display Covid data on a map to get a first sight\n",
    "\n",
    "### The larger is the point on the map  the higher is the Postive/Test ratio ( Ratio built on mean value of all the Covid data available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create map of New York using latitude and longitude values\n",
    "latitude = 46.156782\n",
    "longitude = 4.924699\n",
    "map_france = folium.Map(location=[latitude, longitude], zoom_start=7) ## map centered on Paris coordinates\n",
    "\n",
    "# add markers to map of France\n",
    "for lat, lng, cityname,zip_code,covid in zip(final_dataset['gps_lat'], final_dataset['gps_lng'], final_dataset['Commune'], final_dataset['zip_code'],final_dataset['PtoT_ratio']):\n",
    "    label = '{}, {}, {:,.3f}'.format(int(zip_code), cityname,covid)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=200*covid,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_france)\n",
    "    \n",
    "map_france\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with data set ( covid + cities pop + coordinates ) for clustering \n",
    "#### Clustering will be based on Cities having the most ptoPop_ratio and their social places \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display (HTML('Normalize (OneHot encoding) the fr_cities_venues data to prepare for Kmean'))\n",
    "\n",
    "# one hot encoding\n",
    "fr_cities_venues_onehot = pd.get_dummies(fr_cities_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "fr_cities_venues_onehot['Commune'] = fr_cities_venues['Commune'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [fr_cities_venues_onehot.columns[-1]] + list(fr_cities_venues_onehot.columns[:-1])\n",
    "fr_cities_venues_onehot = fr_cities_venues_onehot[fixed_columns]\n",
    "\n",
    "#display (fr_cities_venues_onehot.head(25))\n",
    "fr_cities_venues_onehot_grouped = fr_cities_venues_onehot.groupby('Commune').mean().reset_index()\n",
    "fr_cities_venues_onehot_grouped.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]\n",
    "\n",
    "num_top_venues = 10\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Commune']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "fr_cities_venues_sorted = pd.DataFrame(columns=columns)\n",
    "fr_cities_venues_sorted['Commune'] = fr_cities_venues_onehot_grouped['Commune']\n",
    "\n",
    "for ind in np.arange(fr_cities_venues_onehot_grouped.shape[0]):\n",
    "    fr_cities_venues_sorted.iloc[ind, 1:] = return_most_common_venues(fr_cities_venues_onehot_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "fr_cities_venues_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "display (HTML('<h3>Run _k_-means to cluster the Commune into N clusters.</h3>'))\n",
    "\n",
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "fr_cities_grouped_clustering = fr_cities_venues_onehot_grouped.drop('Commune', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(fr_cities_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "try:\n",
    "    fr_cities_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "except ValueError:\n",
    "    print (\" cluster labels already there you need to recreate the DF fr_cities_venues_sorted above !!  \")\n",
    "\n",
    "\n",
    "#fr_cities_merged = final_dataset[['Commune','Commune Latitude','Commune Longitude','Venue','pop','PtoT_ratio','dep']]\n",
    "fr_cities_merged = final_dataset[['Commune','Commune Latitude','Commune Longitude','Venue','PtoT_ratio','dep']]\n",
    "\n",
    "\n",
    "# merge manhattan_grouped with manhattan_data to add latitude/longitude for each neighborhood\n",
    "fr_cities_merged = fr_cities_merged.join(fr_cities_venues_sorted.set_index('Commune'), on='Commune')\n",
    "\n",
    "fr_cities_merged # check the last columns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display (HTML('<h3>Visual the cluster in a map.</h3>'))\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# create map\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=7)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster, covid in zip(fr_cities_merged['Commune Latitude'], fr_cities_merged['Commune Longitude'], fr_cities_merged['Commune'], fr_cities_merged['Cluster Labels'], fr_cities_merged['PtoT_ratio']):\n",
    "    label = '{}, Cluster {}, PtoT {:,.3f}'.format(str(poi), cluster,covid)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=200*covid,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cluster_list = fr_cities_merged['Cluster Labels'].unique()\n",
    "for cluster in cluster_list:\n",
    "    display (HTML('<h3>Cluster '+str(cluster)+'</h3>'))\n",
    "    display (fr_cities_merged.loc[fr_cities_merged['Cluster Labels'] == cluster].agg({'PtoT_ratio':'mean'}).reset_index())\n",
    "    display (fr_cities_merged.loc[fr_cities_merged['Cluster Labels'] == cluster])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
